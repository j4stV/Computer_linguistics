"""–ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ RAG —Å–∏—Å—Ç–µ–º—ã - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–¥ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞ —Å –º–∞–ª–µ–Ω—å–∫–æ–π –º–æ–¥–µ–ª—å—é."""

from rag.rag_system import RAGSystem

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (–∏–∑–º–µ–Ω–∏—Ç–µ –ø–æ–¥ —Å–≤–æ–∏ —Ñ–∞–π–ª—ã)
ONTOLOGY_FILES = [
    r"c:\Users\just_\Downloads\graph(5).json",
    r"c:\Users\just_\Downloads\graph (2).json"
]

# –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –î–õ–Ø –ë–´–°–¢–†–û–ì–û –ó–ê–ü–£–°–ö–ê:
# 1. –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫—É—é –º–æ–¥–µ–ª—å (TinyLlama ~1.1B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
#    –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã: "microsoft/Phi-3-mini-4k-instruct", "gpt2", "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
SMALL_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# 2. –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
N_NODES = 3  # –ü–µ—Ä–≤—ã–π –ø–æ–∏—Å–∫
M_NODES = 2  # –í—Ç–æ—Ä–æ–π –ø–æ–∏—Å–∫

print("üöÄ –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ RAG —Å –º–∞–ª–µ–Ω—å–∫–æ–π –º–æ–¥–µ–ª—å—é...")
print(f"üì¶ –ú–æ–¥–µ–ª—å: {SMALL_MODEL}")
print(f"‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞: N={N_NODES}, M={M_NODES}\n")

# –ë—ã—Å—Ç—Ä–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
rag = RAGSystem(
    ontology_files=ONTOLOGY_FILES,
    llm_model_name=SMALL_MODEL,
    cache_dir="./rag_cache",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    n_nodes=N_NODES,
    m_nodes=M_NODES
)

# –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç—ã (—É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
# rag.llm_generator.max_length = 256  # –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ = –±—ã—Å—Ç—Ä–µ–µ
# rag.llm_generator.temperature = 0.3  # –ú–µ–Ω—å—à–µ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ = –±—ã—Å—Ç—Ä–µ–µ

# –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å
question = "–ö—Ç–æ –∑–∞–ø—É—Å—Ç–∏–ª EXP-001?"
print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {question}\n")

# –ü–æ–ª—É—á–∏—Ç–µ –æ—Ç–≤–µ—Ç
answer = rag.query(question, verbose=False)
print(f"‚úÖ –û—Ç–≤–µ—Ç: {answer}")

